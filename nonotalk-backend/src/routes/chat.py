from flask import Blueprint, request, jsonify, session, Response, stream_with_context
from src.models.user import db, User, Conversation, Message, CrisisAlert
from datetime import datetime
import os
import openai
import re
import threading
import time

chat_bp = Blueprint('chat', __name__)

# Configuration OpenAI
from openai import OpenAI
OPENAI_API_KEY = os.getenv('OPENAI_API_KEY', 'sk-fake-key')
OPENAI_API_BASE = os.getenv('OPENAI_API_BASE', 'https://api.openai.com/v1')
client = OpenAI(api_key=OPENAI_API_KEY, base_url=OPENAI_API_BASE)
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage

# Warmup OpenAI model/connection to reduce first-token latency
_warmup_started = False

def _warm_openai_once():
    try:
        model_name = os.getenv('OPENAI_CHAT_MODEL', 'gpt-4o-mini')
        # Minimal prompt to establish TLS and prime model
        client.chat.completions.create(
            model=model_name,
            messages=[{"role": "system", "content": "ping"}],
            max_tokens=1,
            temperature=0
        )
    except Exception as e:
        print(f"[backend] OpenAI warmup failed: {e}")

def start_openai_warmup():
    global _warmup_started
    if _warmup_started:
        return
    _warmup_started = True
    try:
        t = threading.Thread(target=_warm_openai_once, daemon=True)
        t.start()
    except Exception as e:
        print(f"[backend] Warmup thread start failed: {e}")

# Trigger warmup at import time
start_openai_warmup()

# Mots-cl√©s de crise
CRISIS_KEYWORDS = os.getenv('CRISIS_KEYWORDS', 'suicide,envie d\'en finir,je veux mourir,plus envie de vivre').split(',')

def detect_crisis(message_content):
    """D√©tecter les mots-cl√©s de crise dans un message"""
    message_lower = message_content.lower()
    for keyword in CRISIS_KEYWORDS:
        if keyword.strip().lower() in message_lower:
            return True
    return False

def get_gpt_response(message, conversation_history=None, emotion=None):
    """Obtenir une r√©ponse de GPT-4 avec m√©moire (LangChain), fallback OpenAI client."""
    try:
        # Construire le prompt syst√®me
        system_prompt = """
Tu es **Nono**, un psychologue virtuel bienveillant, √† l‚Äô√©coute, empathique et professionnel.  
Tu aides la personne √† exprimer ce qu‚Äôelle ressent, √† comprendre ses √©motions, et √† retrouver de la clart√©.  
Tu parles toujours avec douceur, respect et s√©rieux, en gardant une approche psychologique r√©elle, pas simpliste.

üß© **Ton r√¥le :**
- Offrir un espace s√ªr o√π la personne peut parler librement, sans jugement.  
- Identifier les √©motions, les besoins, et les pens√©es sous-jacentes.  
- Poser des questions ouvertes pour aider la personne √† r√©fl√©chir √† elle-m√™me.  
- Guider la personne √† prendre conscience de ce qu‚Äôelle vit, et √† trouver ses propres solutions.  

üí¨ **Style de r√©ponse :**
- Parle avec empathie et profondeur, comme un vrai psychologue.  
- Utilise des phrases naturelles, bien formul√©es, sans ton robotique.  
- Chaque r√©ponse doit comporter **une reconnaissance √©motionnelle** + **une reformulation** + **une ouverture ou question douce**.  
- 3 √† 5 phrases maximum.  
- Tutoye ou vouvoie selon le ton de la conversation (tu peux alterner selon le contexte √©motionnel).  

üå± **Exemples :**
- ‚ÄúJe comprends, tu traverses quelque chose de difficile. Ce que tu ressens est compl√®tement l√©gitime.  
  Qu‚Äôest-ce qui, selon toi, te p√®se le plus en ce moment ?‚Äù  
- ‚ÄúOn dirait qu‚Äôil y a beaucoup de tension int√©rieure. Parle-moi un peu de ce que tu ressens physiquement quand √ßa arrive.‚Äù  
- ‚ÄúTu sembles en lutte avec toi-m√™me, entre ce que tu ressens et ce que tu crois devoir faire. On peut essayer d‚Äôexplorer √ßa ensemble.‚Äù  

‚öñÔ∏è **Principes √† respecter :**
- Sois empathique, jamais jugeant.  
- Ne donne pas de conseils directs ou d‚Äôordres (‚Äútu devrais‚Ä¶‚Äù).  
- Ne te pr√©sente pas comme un ami, mais comme un professionnel √† l‚Äô√©coute.  
- Reste confidentiel, calme et centr√© sur la personne.  
- Si la situation semble grave ou dangereuse, oriente vers une aide r√©elle (ex: contacter un proche ou un professionnel).  

üí° **Ta mission :**
Aider chaque personne √† se comprendre, √† se sentir entendue, et √† reprendre contact avec ses √©motions profondes.
Nono garde toujours la m√©moire des √©changes pr√©c√©dents pour maintenir une continuit√© th√©rapeutique naturelle et coh√©rente.

"""

        if emotion:
            system_prompt += f"\n\n√âmotion d√©tect√©e dans la voix: {emotion}. Adapte ton ton en cons√©quence."

        # 1) Tentative avec LangChain (m√©moire par conversation)
        try:
            lc_messages = [SystemMessage(content=system_prompt)]
            if conversation_history:
                # Charger plus d'historique pour une meilleure m√©moire (sans rien supprimer en base)
                for msg in conversation_history[-50:]:
                    if msg.is_user:
                        lc_messages.append(HumanMessage(content=msg.content))
                    else:
                        lc_messages.append(AIMessage(content=msg.content))
            lc_messages.append(HumanMessage(content=message))

            llm = ChatOpenAI(
                model=os.getenv('OPENAI_CHAT_MODEL', 'gpt-4o-mini'),
                openai_api_key=OPENAI_API_KEY,
                base_url=OPENAI_API_BASE,
                temperature=0.7,
                max_tokens=150,
            )
            result = llm.invoke(lc_messages)
            return (result.content or "").strip()
        except Exception:
            # 2) Fallback vers le client OpenAI natif si LangChain n'est pas dispo
            messages = [{"role": "system", "content": system_prompt}]
            if conversation_history:
                for msg in conversation_history[-6:]:  # ancienne logique minimale
                    role = "user" if msg.is_user else "assistant"
                    messages.append({"role": role, "content": msg.content})
            messages.append({"role": "user", "content": message})

            response = client.chat.completions.create(
                model=os.getenv('OPENAI_CHAT_MODEL', 'gpt-4o-mini'),
                messages=messages,
                max_tokens=150,
                temperature=0.7
            )
            return response.choices[0].message.content.strip()

    except Exception as e:
        return f"D√©sol√©, je rencontre un probl√®me technique. Peux-tu r√©essayer ? (Erreur: {str(e)})"

@chat_bp.route('/conversations', methods=['GET'])
def get_conversations():
    """R√©cup√©rer toutes les conversations de l'utilisateur"""
    user_id = session.get('user_id')
    if not user_id:
        return jsonify({'error': 'Non connect√©'}), 401

    conversations = Conversation.query.filter_by(user_id=user_id).order_by(Conversation.updated_at.desc()).all()
    
    return jsonify({
        'conversations': [conv.to_dict() for conv in conversations]
    }), 200

@chat_bp.route('/conversations', methods=['POST'])
def create_conversation():
    """Cr√©er une nouvelle conversation"""
    user_id = session.get('user_id')
    if not user_id:
        return jsonify({'error': 'Non connect√©'}), 401

    data = request.get_json()
    title = data.get('title', 'Nouvelle conversation')

    conversation = Conversation(
        user_id=user_id,
        title=title
    )

    db.session.add(conversation)
    db.session.commit()

    return jsonify({
        'message': 'Conversation cr√©√©e',
        'conversation': conversation.to_dict()
    }), 201

@chat_bp.route('/conversations/<int:conversation_id>/messages', methods=['GET'])
def get_messages(conversation_id):
    """R√©cup√©rer les messages d'une conversation (supporte ?limit=10 pour les N derniers)."""
    user_id = session.get('user_id')
    if not user_id:
        return jsonify({'error': 'Non connect√©'}), 401

    conversation = Conversation.query.filter_by(id=conversation_id, user_id=user_id).first()
    if not conversation:
        return jsonify({'error': 'Conversation non trouv√©e'}), 404

    limit = request.args.get('limit', type=int)
    base_query = Message.query.filter_by(conversation_id=conversation_id)

    if limit:
        # Prendre les N plus r√©cents puis remettre en ordre chronologique
        recent = base_query.order_by(Message.timestamp.desc()).limit(limit).all()
        messages = list(reversed(recent))
    else:
        messages = base_query.order_by(Message.timestamp.asc()).all()

    return jsonify({
        'messages': [msg.to_dict() for msg in messages]
    }), 200

@chat_bp.route('/conversations/<int:conversation_id>/send', methods=['POST'])
def send_message(conversation_id):
    """Envoyer un message dans une conversation"""
    print(f"[backend] send_message called: conv_id={conversation_id}, session_user={session.get('user_id')}")
    user_id = session.get('user_id')
    if not user_id:
        print("[backend] 401 Non connect√©")
        return jsonify({'error': 'Non connect√©'}), 401

    # V√©rifier le quota
    user = User.query.get(user_id)
    if not user or user.quota_remaining <= 0:
        return jsonify({
            'error': 'Quota √©puis√©',
            'message': 'Tu as atteint ta limite gratuite. Invite un ami pour d√©bloquer +5 √©changes gratuits pour chacun üéÅ'
        }), 403

    # V√©rifier la conversation
    conversation = Conversation.query.filter_by(id=conversation_id, user_id=user_id).first()
    if not conversation:
        return jsonify({'error': 'Conversation non trouv√©e'}), 404

    data = request.get_json()
    message_content = data.get('message', '').strip()
    emotion = data.get('emotion')

    if not message_content:
        return jsonify({'error': 'Message vide'}), 400

    # D√©tecter les mots-cl√©s de crise
    if detect_crisis(message_content):
        # Enregistrer l'alerte de crise
        crisis_alert = CrisisAlert(
            user_id=user_id,
            message_content=message_content
        )
        db.session.add(crisis_alert)
        db.session.commit()

        # Retourner le message d'urgence
        emergency_message = """üÜò Je suis l√† pour t'√©couter, mais si tu es en danger, contacte imm√©diatement :
üìû 112
‚òéÔ∏è SOS Suicide : 01 45 39 40 00 (gratuit, 24h/24)"""

        return jsonify({
            'crisis_detected': True,
            'emergency_message': emergency_message,
            'message': 'Mots-cl√©s de crise d√©tect√©s'
        }), 200

    try:
        # Sauvegarder le message utilisateur
        user_message = Message(
            conversation_id=conversation_id,
            content=message_content,
            is_user=True,
            emotion_detected=emotion
        )
        db.session.add(user_message)

        # R√©cup√©rer l'historique pour le contexte
        conversation_history = Message.query.filter_by(conversation_id=conversation_id).order_by(Message.timestamp.asc()).all()

        # Obtenir la r√©ponse de l'IA
        ai_response = get_gpt_response(message_content, conversation_history, emotion)

        # Sauvegarder la r√©ponse de l'IA
        ai_message = Message(
            conversation_id=conversation_id,
            content=ai_response,
            is_user=False
        )
        db.session.add(ai_message)

        # Utiliser un quota
        user.use_quota()

        # Mettre √† jour la conversation
        conversation.updated_at = datetime.utcnow()
        if not conversation.title or conversation.title == 'Nouvelle conversation':
            # G√©n√©rer un titre bas√© sur le premier message
            conversation.title = message_content[:50] + ('...' if len(message_content) > 50 else '')

        db.session.commit()

        return jsonify({
            'user_message': user_message.to_dict(),
            'ai_message': ai_message.to_dict(),
            'quota_remaining': user.quota_remaining
        }), 200

    except Exception as e:
        db.session.rollback()
        return jsonify({'error': f'Erreur lors de l\'envoi: {str(e)}'}), 500

@chat_bp.route('/conversations/<int:conversation_id>/send-stream', methods=['POST'])
def send_message_stream(conversation_id):
    """Envoyer un message en mode streaming (SSE-like) pour d√©marrer la r√©ponse plus t√¥t c√¥t√© front."""
    import json
    from flask import stream_with_context

    print(f"[backend] send_message_stream called: conv_id={conversation_id}, session_user={session.get('user_id')}")
    user_id = session.get('user_id')
    if not user_id:
        return jsonify({'error': 'Non connect√©'}), 401

    # V√©rifier le quota
    user = User.query.get(user_id)
    if not user or user.quota_remaining <= 0:
        return jsonify({
            'error': 'Quota √©puis√©',
            'message': 'Tu as atteint ta limite gratuite. Invite un ami pour d√©bloquer +5 √©changes gratuits pour chacun üéÅ'
        }), 403

    # V√©rifier la conversation
    conversation = Conversation.query.filter_by(id=conversation_id, user_id=user_id).first()
    if not conversation:
        return jsonify({'error': 'Conversation non trouv√©e'}), 404

    data = request.get_json()
    message_content = (data.get('message') or '').strip()
    emotion = data.get('emotion')

    if not message_content:
        return jsonify({'error': 'Message vide'}), 400

    # Sauvegarder imm√©diatement le message utilisateur
    user_message = Message(
        conversation_id=conversation_id,
        content=message_content,
        is_user=True,
        emotion_detected=emotion
    )
    db.session.add(user_message)
    db.session.flush()  # id dispo sans commit

    # Pr√©parer le contexte (DB-level limit pour r√©duire la latence)
    recent = Message.query.filter_by(conversation_id=conversation_id).order_by(Message.timestamp.desc()).limit(8).all()
    conversation_history = list(reversed(recent))

    # Construire le prompt syst√®me (m√™me logique que get_gpt_response)
    system_prompt = """
Tu es **Nono**, un psychologue virtuel bienveillant, √† l‚Äô√©coute, empathique et professionnel.  
Tu aides la personne √† exprimer ce qu‚Äôelle ressent, √† comprendre ses √©motions, et √† retrouver de la clart√©.  
Tu parles toujours avec douceur, respect et s√©rieux, en gardant une approche psychologique r√©elle, pas simpliste.

üß© Ton r√¥le :
- Offrir un espace s√ªr o√π la personne peut parler librement, sans jugement.  
- Identifier les √©motions, les besoins, et les pens√©es sous-jacentes.  
- Poser des questions ouvertes pour aider la personne √† r√©fl√©chir √† elle-m√™me.  
- Guider la personne √† prendre conscience de ce qu‚Äôelle vit, et √† trouver ses propres solutions.  

üí¨ Style de r√©ponse :
- Parle avec empathie et profondeur, comme un vrai psychologue.  
- Utilise des phrases naturelles, bien formul√©es, sans ton robotique.  
- Chaque r√©ponse doit comporter une reconnaissance √©motionnelle + une reformulation + une ouverture ou question douce.  
- 3 √† 5 phrases maximum.
"""
    if emotion:
        system_prompt += f"\n\n√âmotion d√©tect√©e dans la voix: {emotion}. Adapte ton ton en cons√©quence."

    def event(data_obj):
        return f"data: {json.dumps(data_obj, ensure_ascii=False)}\n\n"

    @stream_with_context
    def generate():
        full_text = ""
        try:
            start_ts = time.time()
            # Construire l'historique pour OpenAI
            messages = [{"role": "system", "content": system_prompt}]
            if conversation_history:
                for msg in conversation_history[-8:]:
                    role = "user" if msg.is_user else "assistant"
                    messages.append({"role": role, "content": msg.content})
            messages.append({"role": "user", "content": message_content})

            # Envoyer un √©v√®nement de d√©marrage (flush imm√©diat)
            yield event({"type": "start"})
            # Padding pour forcer le flush sur certains proxys/clients
            yield ":" + (" " * 2048) + "\n\n"

            # D√©marrer le stream OpenAI
            model_name = os.getenv('OPENAI_CHAT_MODEL', 'gpt-4o-mini')
            stream = client.chat.completions.create(
                model=model_name,
                messages=messages,
                max_tokens=180,
                temperature=0.7,
                stream=True,
            )
            first_piece_sent = False

            for chunk in stream:
                try:
                    choice = (chunk.choices or [None])[0]
                    delta = getattr(choice, "delta", None)
                    piece = getattr(delta, "content", None) if delta else None
                    if piece:
                        if not first_piece_sent:
                            try:
                                yield event({"type": "first_delta_ms", "ms": int((time.time() - start_ts) * 1000)})
                            except Exception:
                                pass
                            first_piece_sent = True
                        full_text += piece
                        yield event({"type": "delta", "content": piece})
                except Exception as iter_err:
                    print("[backend] stream iteration error:", iter_err)

            # Fin du stream -> persister la r√©ponse, MAJ quota
            ai_message = Message(
                conversation_id=conversation_id,
                content=full_text.strip(),
                is_user=False
            )
            db.session.add(ai_message)

            # Utiliser un quota
            user.use_quota()

            # MAJ conversation
            conversation.updated_at = datetime.utcnow()
            if not conversation.title or conversation.title == 'Nouvelle conversation':
                conversation.title = message_content[:50] + ('...' if len(message_content) > 50 else '')

            db.session.commit()

            # Ev√®nement final avec metadata
            yield event({
                "type": "done",
                "text": full_text.strip(),
                "user_message": user_message.to_dict(),
                "ai_message": ai_message.to_dict(),
                "quota_remaining": user.quota_remaining
            })

        except Exception as e:
            db.session.rollback()
            yield event({"type": "error", "error": str(e)})

    resp = Response(generate(), mimetype='text/event-stream')
    resp.headers['Cache-Control'] = 'no-cache, no-transform'
    resp.headers['X-Accel-Buffering'] = 'no'
    resp.headers['Connection'] = 'keep-alive'
    resp.headers['Content-Type'] = 'text/event-stream; charset=utf-8'
    return resp

@chat_bp.route('/conversations/<int:conversation_id>/upload-image', methods=['POST'])
def upload_image(conversation_id):
    """Upload et analyse d'image avec GPT Vision"""
    user_id = session.get('user_id')
    if not user_id:
        return jsonify({'error': 'Non connect√©'}), 401

    # V√©rifier le quota
    user = User.query.get(user_id)
    if not user or user.quota_remaining <= 0:
        return jsonify({
            'error': 'Quota √©puis√©',
            'message': 'Tu as atteint ta limite gratuite. Invite un ami pour d√©bloquer +5 √©changes gratuits pour chacun üéÅ'
        }), 403

    # V√©rifier la conversation
    conversation = Conversation.query.filter_by(id=conversation_id, user_id=user_id).first()
    if not conversation:
        return jsonify({'error': 'Conversation non trouv√©e'}), 404

    if 'image' not in request.files:
        return jsonify({'error': 'Aucune image fournie'}), 400

    try:
        image_file = request.files['image']
        if image_file.filename == '':
            return jsonify({'error': 'Aucune image s√©lectionn√©e'}), 400

        # Sauvegarder l'image temporairement
        upload_dir = os.path.join(os.path.dirname(__file__), '..', 'static', 'uploads')
        os.makedirs(upload_dir, exist_ok=True)
        
        filename = f"user_{user_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{image_file.filename}"
        image_path = os.path.join(upload_dir, filename)
        image_file.save(image_path)

        # Cr√©er un message avec l'image
        image_message = Message(
            conversation_id=conversation_id,
            content="[Image partag√©e]",
            is_user=True,
            image_path=f"uploads/{filename}"
        )
        db.session.add(image_message)

        # Analyser l'image avec GPT Vision (simulation pour le moment)
        vision_prompt = """
L'utilisateur t'a envoy√© une image pour exprimer son √©tat √©motionnel du moment. 
Tu es un psychologue √† l'√©coute, calme et bienveillant. 
Observe cette image comme une fen√™tre sur son ressenti int√©rieur. 
R√©ponds avec empathie et profondeur, sans d√©crire visuellement l‚Äôimage. 
Si tu per√ßois de la solitude, du stress ou de la tristesse, reformule ce que tu ressens et offre un message de soutien doux. 
Ta r√©ponse doit √™tre br√®ve (2 √† 3 phrases) et empreinte d‚Äôhumanit√©.
"""

        # Pour le moment, r√©ponse g√©n√©rique (√† remplacer par GPT Vision)
        ai_response = "Merci pour cette image. Elle semble refl√©ter un √©tat int√©rieur particulier. Qu‚Äôest-ce qui t‚Äôa pouss√© √† la choisir ou √† la partager aujourd‚Äôhui ?"

        # Sauvegarder la r√©ponse de l'IA
        ai_message = Message(
            conversation_id=conversation_id,
            content=ai_response,
            is_user=False
        )
        db.session.add(ai_message)

        # Utiliser un quota
        user.use_quota()

        # Mettre √† jour la conversation
        conversation.updated_at = datetime.utcnow()

        db.session.commit()

        return jsonify({
            'image_message': image_message.to_dict(),
            'ai_message': ai_message.to_dict(),
            'quota_remaining': user.quota_remaining
        }), 200

    except Exception as e:
        db.session.rollback()
        return jsonify({'error': f'Erreur lors de l\'upload: {str(e)}'}), 500

@chat_bp.route('/crisis/acknowledge', methods=['POST'])
def acknowledge_crisis():
    """Marquer qu'on a compris le message de crise"""
    user_id = session.get('user_id')
    if not user_id:
        return jsonify({'error': 'Non connect√©'}), 401

    # Marquer les alertes de crise comme r√©solues
    CrisisAlert.query.filter_by(user_id=user_id, resolved=False).update({'resolved': True})
    db.session.commit()

    return jsonify({'message': 'Crise acknowledg√©e'}), 200
